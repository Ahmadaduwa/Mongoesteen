{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f77c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:08:13,142 INFO: Scaler saved to ./volume_scaler_finetune.save\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:08:13,989 INFO: Training regression head...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\Python\\Project\\Mongoesteen\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 19.9668 - mean_absolute_error: 1.5209\n",
      "Epoch 1: val_loss improved from None to 14.55498, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:08:33,764 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 18.4984 - mean_absolute_error: 1.4756 - val_loss: 14.5550 - val_mean_absolute_error: 0.7517 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 13.3897 - mean_absolute_error: 0.7178\n",
      "Epoch 2: val_loss improved from 14.55498 to 9.94244, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:08:51,099 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - loss: 12.2941 - mean_absolute_error: 0.7069 - val_loss: 9.9424 - val_mean_absolute_error: 0.7039 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 9.1539 - mean_absolute_error: 0.6504\n",
      "Epoch 3: val_loss improved from 9.94244 to 6.88309, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:09:07,740 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - loss: 8.4304 - mean_absolute_error: 0.6226 - val_loss: 6.8831 - val_mean_absolute_error: 0.6443 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 6.3436 - mean_absolute_error: 0.5412\n",
      "Epoch 4: val_loss improved from 6.88309 to 4.94339, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:09:25,025 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - loss: 5.9157 - mean_absolute_error: 0.5612 - val_loss: 4.9434 - val_mean_absolute_error: 0.5660 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 4.6025 - mean_absolute_error: 0.4916\n",
      "Epoch 5: val_loss improved from 4.94339 to 3.77316, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:09:48,648 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - loss: 4.3628 - mean_absolute_error: 0.5341 - val_loss: 3.7732 - val_mean_absolute_error: 0.5506 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 3.5653 - mean_absolute_error: 0.5144\n",
      "Epoch 6: val_loss improved from 3.77316 to 2.98574, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:10:14,589 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 3.3998 - mean_absolute_error: 0.5362 - val_loss: 2.9857 - val_mean_absolute_error: 0.5462 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.8156 - mean_absolute_error: 0.4826\n",
      "Epoch 7: val_loss improved from 2.98574 to 2.44102, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:10:43,042 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - loss: 2.7017 - mean_absolute_error: 0.4926 - val_loss: 2.4410 - val_mean_absolute_error: 0.5496 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.3305 - mean_absolute_error: 0.5128\n",
      "Epoch 8: val_loss improved from 2.44102 to 2.05074, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:11:10,822 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 2.2645 - mean_absolute_error: 0.5448 - val_loss: 2.0507 - val_mean_absolute_error: 0.5294 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.9273 - mean_absolute_error: 0.4540\n",
      "Epoch 9: val_loss improved from 2.05074 to 1.74798, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:11:37,330 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.8660 - mean_absolute_error: 0.4691 - val_loss: 1.7480 - val_mean_absolute_error: 0.5269 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.6733 - mean_absolute_error: 0.4905\n",
      "Epoch 10: val_loss improved from 1.74798 to 1.53481, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:12:04,023 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.6225 - mean_absolute_error: 0.5010 - val_loss: 1.5348 - val_mean_absolute_error: 0.5623 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.4029 - mean_absolute_error: 0.4453\n",
      "Epoch 11: val_loss improved from 1.53481 to 1.31329, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:12:30,588 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.3771 - mean_absolute_error: 0.4622 - val_loss: 1.3133 - val_mean_absolute_error: 0.5132 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.2137 - mean_absolute_error: 0.4282\n",
      "Epoch 12: val_loss improved from 1.31329 to 1.15535, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:12:57,396 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.1924 - mean_absolute_error: 0.4518 - val_loss: 1.1553 - val_mean_absolute_error: 0.5261 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 1.0966 - mean_absolute_error: 0.4679\n",
      "Epoch 13: val_loss improved from 1.15535 to 1.07256, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:13:24,470 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 1.0459 - mean_absolute_error: 0.4397 - val_loss: 1.0726 - val_mean_absolute_error: 0.5634 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.9935 - mean_absolute_error: 0.5031\n",
      "Epoch 14: val_loss improved from 1.07256 to 0.96224, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:13:51,342 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.9513 - mean_absolute_error: 0.4821 - val_loss: 0.9622 - val_mean_absolute_error: 0.6118 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.8777 - mean_absolute_error: 0.4889\n",
      "Epoch 15: val_loss improved from 0.96224 to 0.84282, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:14:08,002 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.8467 - mean_absolute_error: 0.4599 - val_loss: 0.8428 - val_mean_absolute_error: 0.5051 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.7757 - mean_absolute_error: 0.4359\n",
      "Epoch 16: val_loss improved from 0.84282 to 0.76458, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:14:30,268 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - loss: 0.7576 - mean_absolute_error: 0.4387 - val_loss: 0.7646 - val_mean_absolute_error: 0.5045 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.6622 - mean_absolute_error: 0.3797\n",
      "Epoch 17: val_loss improved from 0.76458 to 0.73071, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:14:56,652 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.6643 - mean_absolute_error: 0.4136 - val_loss: 0.7307 - val_mean_absolute_error: 0.5316 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5978 - mean_absolute_error: 0.3853\n",
      "Epoch 18: val_loss improved from 0.73071 to 0.64473, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:15:25,174 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - loss: 0.6065 - mean_absolute_error: 0.4148 - val_loss: 0.6447 - val_mean_absolute_error: 0.4949 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5729 - mean_absolute_error: 0.4177\n",
      "Epoch 19: val_loss improved from 0.64473 to 0.60724, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:16:05,066 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - loss: 0.5728 - mean_absolute_error: 0.4271 - val_loss: 0.6072 - val_mean_absolute_error: 0.4982 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5255 - mean_absolute_error: 0.4277\n",
      "Epoch 20: val_loss improved from 0.60724 to 0.57914, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:16:32,700 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - loss: 0.5362 - mean_absolute_error: 0.4434 - val_loss: 0.5791 - val_mean_absolute_error: 0.5058 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5193 - mean_absolute_error: 0.4423\n",
      "Epoch 21: val_loss improved from 0.57914 to 0.54517, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:16:59,832 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5130 - mean_absolute_error: 0.4443 - val_loss: 0.5452 - val_mean_absolute_error: 0.5226 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5004 - mean_absolute_error: 0.4698\n",
      "Epoch 22: val_loss improved from 0.54517 to 0.51956, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:17:26,210 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.4729 - mean_absolute_error: 0.4427 - val_loss: 0.5196 - val_mean_absolute_error: 0.5537 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.4649 - mean_absolute_error: 0.4733\n",
      "Epoch 23: val_loss did not improve from 0.51956\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.4586 - mean_absolute_error: 0.4686 - val_loss: 0.5407 - val_mean_absolute_error: 0.6247 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.4619 - mean_absolute_error: 0.4979\n",
      "Epoch 24: val_loss improved from 0.51956 to 0.46733, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:18:18,542 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.4443 - mean_absolute_error: 0.4602 - val_loss: 0.4673 - val_mean_absolute_error: 0.5345 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.4213 - mean_absolute_error: 0.4692\n",
      "Epoch 25: val_loss improved from 0.46733 to 0.43080, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:18:45,117 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.4114 - mean_absolute_error: 0.4559 - val_loss: 0.4308 - val_mean_absolute_error: 0.5006 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3540 - mean_absolute_error: 0.3891\n",
      "Epoch 26: val_loss improved from 0.43080 to 0.40941, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:19:11,754 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.3695 - mean_absolute_error: 0.4135 - val_loss: 0.4094 - val_mean_absolute_error: 0.4845 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3554 - mean_absolute_error: 0.4052\n",
      "Epoch 27: val_loss did not improve from 0.40941\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.3508 - mean_absolute_error: 0.4007 - val_loss: 0.4173 - val_mean_absolute_error: 0.5448 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3772 - mean_absolute_error: 0.4616\n",
      "Epoch 28: val_loss improved from 0.40941 to 0.39345, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:20:03,400 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.3432 - mean_absolute_error: 0.4176 - val_loss: 0.3935 - val_mean_absolute_error: 0.5021 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3071 - mean_absolute_error: 0.3927\n",
      "Epoch 29: val_loss did not improve from 0.39345\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.3215 - mean_absolute_error: 0.4169 - val_loss: 0.3948 - val_mean_absolute_error: 0.5122 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3192 - mean_absolute_error: 0.4165\n",
      "Epoch 30: val_loss improved from 0.39345 to 0.37555, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:20:42,987 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.3067 - mean_absolute_error: 0.4099 - val_loss: 0.3756 - val_mean_absolute_error: 0.5076 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3154 - mean_absolute_error: 0.4229\n",
      "Epoch 31: val_loss did not improve from 0.37555\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.3106 - mean_absolute_error: 0.4278 - val_loss: 0.3843 - val_mean_absolute_error: 0.5121 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2856 - mean_absolute_error: 0.3953\n",
      "Epoch 32: val_loss improved from 0.37555 to 0.35563, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:21:35,340 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2897 - mean_absolute_error: 0.4058 - val_loss: 0.3556 - val_mean_absolute_error: 0.4948 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2723 - mean_absolute_error: 0.3878\n",
      "Epoch 33: val_loss did not improve from 0.35563\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.2895 - mean_absolute_error: 0.4168 - val_loss: 0.3708 - val_mean_absolute_error: 0.5281 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2653 - mean_absolute_error: 0.3908\n",
      "Epoch 34: val_loss improved from 0.35563 to 0.33484, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:22:26,904 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2729 - mean_absolute_error: 0.4073 - val_loss: 0.3348 - val_mean_absolute_error: 0.4848 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2713 - mean_absolute_error: 0.3964\n",
      "Epoch 35: val_loss did not improve from 0.33484\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.2772 - mean_absolute_error: 0.4096 - val_loss: 0.3602 - val_mean_absolute_error: 0.5304 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2702 - mean_absolute_error: 0.4149\n",
      "Epoch 36: val_loss did not improve from 0.33484\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.2506 - mean_absolute_error: 0.3843 - val_loss: 0.3600 - val_mean_absolute_error: 0.5257 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2292 - mean_absolute_error: 0.3631\n",
      "Epoch 37: val_loss improved from 0.33484 to 0.32056, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:23:32,166 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.2501 - mean_absolute_error: 0.3868 - val_loss: 0.3206 - val_mean_absolute_error: 0.4946 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2488 - mean_absolute_error: 0.3956\n",
      "Epoch 38: val_loss did not improve from 0.32056\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2492 - mean_absolute_error: 0.4014 - val_loss: 0.4587 - val_mean_absolute_error: 0.6610 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2963 - mean_absolute_error: 0.4690\n",
      "Epoch 39: val_loss did not improve from 0.32056\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.2858 - mean_absolute_error: 0.4637 - val_loss: 0.3964 - val_mean_absolute_error: 0.5926 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2558 - mean_absolute_error: 0.4108\n",
      "Epoch 40: val_loss did not improve from 0.32056\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2534 - mean_absolute_error: 0.4057 - val_loss: 0.3296 - val_mean_absolute_error: 0.5408 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2509 - mean_absolute_error: 0.4227\n",
      "Epoch 41: val_loss did not improve from 0.32056\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2466 - mean_absolute_error: 0.4143 - val_loss: 0.3350 - val_mean_absolute_error: 0.5133 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2677 - mean_absolute_error: 0.4393\n",
      "Epoch 42: val_loss did not improve from 0.32056\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.2575 - mean_absolute_error: 0.4290 - val_loss: 0.3211 - val_mean_absolute_error: 0.5106 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2755 - mean_absolute_error: 0.4554\n",
      "Epoch 43: val_loss improved from 0.32056 to 0.29356, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:26:07,740 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - loss: 0.2589 - mean_absolute_error: 0.4369 - val_loss: 0.2936 - val_mean_absolute_error: 0.4812 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2331 - mean_absolute_error: 0.3900\n",
      "Epoch 44: val_loss did not improve from 0.29356\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 22s/step - loss: 0.2369 - mean_absolute_error: 0.3909 - val_loss: 0.3149 - val_mean_absolute_error: 0.5018 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - loss: 0.2612 - mean_absolute_error: 0.4390 \n",
      "Epoch 45: val_loss did not improve from 0.29356\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 15s/step - loss: 0.2489 - mean_absolute_error: 0.4284 - val_loss: 0.3362 - val_mean_absolute_error: 0.5307 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2418 - mean_absolute_error: 0.4219\n",
      "Epoch 46: val_loss did not improve from 0.29356\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - loss: 0.2423 - mean_absolute_error: 0.4271 - val_loss: 0.3000 - val_mean_absolute_error: 0.4971 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2181 - mean_absolute_error: 0.3887\n",
      "Epoch 47: val_loss improved from 0.29356 to 0.27800, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:33:22,108 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - loss: 0.2300 - mean_absolute_error: 0.4026 - val_loss: 0.2780 - val_mean_absolute_error: 0.4684 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2316 - mean_absolute_error: 0.4110\n",
      "Epoch 48: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - loss: 0.2383 - mean_absolute_error: 0.4167 - val_loss: 0.2915 - val_mean_absolute_error: 0.5078 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2360 - mean_absolute_error: 0.4273\n",
      "Epoch 49: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3s/step - loss: 0.2142 - mean_absolute_error: 0.3914 - val_loss: 0.2959 - val_mean_absolute_error: 0.4967 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2032 - mean_absolute_error: 0.3653\n",
      "Epoch 50: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3s/step - loss: 0.2060 - mean_absolute_error: 0.3670 - val_loss: 0.3141 - val_mean_absolute_error: 0.5185 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2146 - mean_absolute_error: 0.3973\n",
      "Epoch 51: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.2265 - mean_absolute_error: 0.4153 - val_loss: 0.2818 - val_mean_absolute_error: 0.4902 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.2018 - mean_absolute_error: 0.3866\n",
      "Epoch 52: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4s/step - loss: 0.2277 - mean_absolute_error: 0.4163 - val_loss: 0.3197 - val_mean_absolute_error: 0.5228 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2094 - mean_absolute_error: 0.3772\n",
      "Epoch 53: val_loss did not improve from 0.27800\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - loss: 0.2251 - mean_absolute_error: 0.4049 - val_loss: 0.2956 - val_mean_absolute_error: 0.5065 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1901 - mean_absolute_error: 0.3612\n",
      "Epoch 54: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.1960 - mean_absolute_error: 0.3656 - val_loss: 0.2811 - val_mean_absolute_error: 0.4783 - learning_rate: 2.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1978 - mean_absolute_error: 0.3764\n",
      "Epoch 55: val_loss did not improve from 0.27800\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.1956 - mean_absolute_error: 0.3796 - val_loss: 0.2815 - val_mean_absolute_error: 0.4845 - learning_rate: 2.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2002 - mean_absolute_error: 0.3798\n",
      "Epoch 56: val_loss improved from 0.27800 to 0.27156, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:38:32,645 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - loss: 0.1806 - mean_absolute_error: 0.3543 - val_loss: 0.2716 - val_mean_absolute_error: 0.4760 - learning_rate: 2.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1799 - mean_absolute_error: 0.3465\n",
      "Epoch 57: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - loss: 0.1938 - mean_absolute_error: 0.3618 - val_loss: 0.2787 - val_mean_absolute_error: 0.4744 - learning_rate: 2.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1909 - mean_absolute_error: 0.3635\n",
      "Epoch 58: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - loss: 0.1852 - mean_absolute_error: 0.3581 - val_loss: 0.2819 - val_mean_absolute_error: 0.4787 - learning_rate: 2.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1841 - mean_absolute_error: 0.3380\n",
      "Epoch 59: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - loss: 0.1868 - mean_absolute_error: 0.3531 - val_loss: 0.2771 - val_mean_absolute_error: 0.4829 - learning_rate: 2.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1910 - mean_absolute_error: 0.3662\n",
      "Epoch 60: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - loss: 0.1821 - mean_absolute_error: 0.3559 - val_loss: 0.2777 - val_mean_absolute_error: 0.4822 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1949 - mean_absolute_error: 0.3686\n",
      "Epoch 61: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - loss: 0.1790 - mean_absolute_error: 0.3514 - val_loss: 0.2719 - val_mean_absolute_error: 0.4909 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1737 - mean_absolute_error: 0.3465\n",
      "Epoch 62: val_loss did not improve from 0.27156\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.1718 - mean_absolute_error: 0.3404 - val_loss: 0.2762 - val_mean_absolute_error: 0.4772 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1772 - mean_absolute_error: 0.3555\n",
      "Epoch 63: val_loss did not improve from 0.27156\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - loss: 0.1767 - mean_absolute_error: 0.3489 - val_loss: 0.2767 - val_mean_absolute_error: 0.4828 - learning_rate: 4.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1695 - mean_absolute_error: 0.3322\n",
      "Epoch 64: val_loss improved from 0.27156 to 0.27083, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:42:17,399 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - loss: 0.1704 - mean_absolute_error: 0.3365 - val_loss: 0.2708 - val_mean_absolute_error: 0.4840 - learning_rate: 4.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1712 - mean_absolute_error: 0.3411\n",
      "Epoch 65: val_loss improved from 0.27083 to 0.26898, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:42:48,399 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - loss: 0.1647 - mean_absolute_error: 0.3272 - val_loss: 0.2690 - val_mean_absolute_error: 0.4798 - learning_rate: 4.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1544 - mean_absolute_error: 0.3095\n",
      "Epoch 66: val_loss improved from 0.26898 to 0.26735, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:43:20,034 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.1686 - mean_absolute_error: 0.3269 - val_loss: 0.2673 - val_mean_absolute_error: 0.4771 - learning_rate: 4.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1618 - mean_absolute_error: 0.3316\n",
      "Epoch 67: val_loss did not improve from 0.26735\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.1515 - mean_absolute_error: 0.3134 - val_loss: 0.2678 - val_mean_absolute_error: 0.4760 - learning_rate: 4.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1980 - mean_absolute_error: 0.3678\n",
      "Epoch 68: val_loss improved from 0.26735 to 0.26665, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:44:22,934 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - loss: 0.1818 - mean_absolute_error: 0.3515 - val_loss: 0.2667 - val_mean_absolute_error: 0.4747 - learning_rate: 4.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1775 - mean_absolute_error: 0.3478\n",
      "Epoch 69: val_loss improved from 0.26665 to 0.26587, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:44:52,537 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - loss: 0.1612 - mean_absolute_error: 0.3304 - val_loss: 0.2659 - val_mean_absolute_error: 0.4769 - learning_rate: 4.0000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.1690 - mean_absolute_error: 0.3301\n",
      "Epoch 70: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 0.1688 - mean_absolute_error: 0.3335 - val_loss: 0.2671 - val_mean_absolute_error: 0.4802 - learning_rate: 4.0000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1430 - mean_absolute_error: 0.2967\n",
      "Epoch 71: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.1545 - mean_absolute_error: 0.3195 - val_loss: 0.2700 - val_mean_absolute_error: 0.4830 - learning_rate: 4.0000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1735 - mean_absolute_error: 0.3437\n",
      "Epoch 72: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - loss: 0.1765 - mean_absolute_error: 0.3437 - val_loss: 0.2682 - val_mean_absolute_error: 0.4804 - learning_rate: 4.0000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1552 - mean_absolute_error: 0.3186\n",
      "Epoch 73: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - loss: 0.1704 - mean_absolute_error: 0.3436 - val_loss: 0.2674 - val_mean_absolute_error: 0.4779 - learning_rate: 4.0000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1826 - mean_absolute_error: 0.3538\n",
      "Epoch 74: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.1706 - mean_absolute_error: 0.3364 - val_loss: 0.2692 - val_mean_absolute_error: 0.4760 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1705 - mean_absolute_error: 0.3369\n",
      "Epoch 75: val_loss did not improve from 0.26587\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1681 - mean_absolute_error: 0.3351 - val_loss: 0.2675 - val_mean_absolute_error: 0.4772 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1726 - mean_absolute_error: 0.3380\n",
      "Epoch 76: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1604 - mean_absolute_error: 0.3224 - val_loss: 0.2675 - val_mean_absolute_error: 0.4774 - learning_rate: 8.0000e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1426 - mean_absolute_error: 0.2917\n",
      "Epoch 77: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1621 - mean_absolute_error: 0.3197 - val_loss: 0.2672 - val_mean_absolute_error: 0.4777 - learning_rate: 8.0000e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1620 - mean_absolute_error: 0.3290\n",
      "Epoch 78: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1552 - mean_absolute_error: 0.3207 - val_loss: 0.2668 - val_mean_absolute_error: 0.4776 - learning_rate: 8.0000e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1629 - mean_absolute_error: 0.3308\n",
      "Epoch 79: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1702 - mean_absolute_error: 0.3420 - val_loss: 0.2666 - val_mean_absolute_error: 0.4769 - learning_rate: 8.0000e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1469 - mean_absolute_error: 0.3133\n",
      "Epoch 80: val_loss did not improve from 0.26587\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - loss: 0.1512 - mean_absolute_error: 0.3183 - val_loss: 0.2665 - val_mean_absolute_error: 0.4763 - learning_rate: 8.0000e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1583 - mean_absolute_error: 0.3150\n",
      "Epoch 81: val_loss did not improve from 0.26587\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 0.1571 - mean_absolute_error: 0.3171 - val_loss: 0.2660 - val_mean_absolute_error: 0.4762 - learning_rate: 8.0000e-06\n",
      "Epoch 81: early stopping\n",
      "Restoring model weights from the end of the best epoch: 69.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:48:11,315 INFO: Fine-tuning last blocks of base model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3109 - mean_absolute_error: 0.5359\n",
      "Epoch 1: val_loss improved from 0.26587 to 0.26467, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:48:40,201 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 0.3190 - mean_absolute_error: 0.5481 - val_loss: 0.2647 - val_mean_absolute_error: 0.4762 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2972 - mean_absolute_error: 0.5250\n",
      "Epoch 2: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2916 - mean_absolute_error: 0.5112 - val_loss: 0.2728 - val_mean_absolute_error: 0.4933 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2804 - mean_absolute_error: 0.5051\n",
      "Epoch 3: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2767 - mean_absolute_error: 0.5025 - val_loss: 0.2914 - val_mean_absolute_error: 0.5217 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2330 - mean_absolute_error: 0.4332\n",
      "Epoch 4: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 0.2404 - mean_absolute_error: 0.4460 - val_loss: 0.3080 - val_mean_absolute_error: 0.5437 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2203 - mean_absolute_error: 0.4200\n",
      "Epoch 5: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2302 - mean_absolute_error: 0.4392 - val_loss: 0.3248 - val_mean_absolute_error: 0.5663 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2284 - mean_absolute_error: 0.4353\n",
      "Epoch 6: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2254 - mean_absolute_error: 0.4252 - val_loss: 0.3277 - val_mean_absolute_error: 0.5685 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2078 - mean_absolute_error: 0.3939\n",
      "Epoch 7: val_loss did not improve from 0.26467\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2156 - mean_absolute_error: 0.4099 - val_loss: 0.3243 - val_mean_absolute_error: 0.5610 - learning_rate: 1.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2170 - mean_absolute_error: 0.4113\n",
      "Epoch 8: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2160 - mean_absolute_error: 0.4142 - val_loss: 0.3132 - val_mean_absolute_error: 0.5485 - learning_rate: 2.0000e-06\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2514 - mean_absolute_error: 0.4541\n",
      "Epoch 9: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2091 - mean_absolute_error: 0.4100 - val_loss: 0.3038 - val_mean_absolute_error: 0.5376 - learning_rate: 2.0000e-06\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1912 - mean_absolute_error: 0.3692\n",
      "Epoch 10: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2039 - mean_absolute_error: 0.3923 - val_loss: 0.2950 - val_mean_absolute_error: 0.5270 - learning_rate: 2.0000e-06\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1779 - mean_absolute_error: 0.3559\n",
      "Epoch 11: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.1920 - mean_absolute_error: 0.3779 - val_loss: 0.2873 - val_mean_absolute_error: 0.5163 - learning_rate: 2.0000e-06\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1982 - mean_absolute_error: 0.3808\n",
      "Epoch 12: val_loss did not improve from 0.26467\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.1897 - mean_absolute_error: 0.3730 - val_loss: 0.2809 - val_mean_absolute_error: 0.5078 - learning_rate: 2.0000e-06\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2055 - mean_absolute_error: 0.3910\n",
      "Epoch 13: val_loss did not improve from 0.26467\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - loss: 0.2138 - mean_absolute_error: 0.4084 - val_loss: 0.2760 - val_mean_absolute_error: 0.5000 - learning_rate: 2.0000e-06\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:52:29,472 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-09-13 21:52:30,849 INFO: ✅ Model saved to ./mangosteen_volume_model_aug_finetune.h5\n",
      "2025-09-13 21:52:30,850 INFO: Best checkpoint: ./checkpoints\\best_20250913-210813.h5\n",
      "2025-09-13 21:52:31,102 INFO: Training plot saved to training_20250913-210813.png\n",
      "2025-09-13 21:52:31,104 INFO: Evaluating on validation set (original volume units)...\n",
      "2025-09-13 21:52:35,298 INFO: Validation MAE (orig units): 12.1681\n",
      "2025-09-13 21:52:35,298 INFO: Validation RMSE (orig units): 17.7695\n",
      "2025-09-13 21:52:35,304 INFO: Saved validation predictions to val_predictions_20250913-210813.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, CSVLogger\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# ---------------- config ----------------\n",
    "DATA_DIR = \"./DataTrain/images\"\n",
    "VOLUME_FILES_DIR = \"./DataTrain/labels\"\n",
    "MODEL_SAVE_PATH = \"./mangosteen_volume_model_aug_finetune.h5\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "SCALER_PATH = \"./volume_scaler_finetune.save\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 100\n",
    "FINE_TUNE_EPOCHS = 100\n",
    "LEARNING_RATE_INITIAL = 1e-3\n",
    "LEARNING_RATE_FINE_TUNE = 1e-8\n",
    "RANDOM_SEED = 42\n",
    "ENABLE_MIXED_PRECISION = False  # เปลี่ยนเป็น True ถ้าต้องการใช้ mixed precision (GPU + ระวัง output dtype)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- reproducibility ----------------\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------- GPU safe config ----------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(\"GPU found. Enabled memory growth.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "\n",
    "if ENABLE_MIXED_PRECISION:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled.\")\n",
    "\n",
    "# ---------------- logging ----------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "# --- augmentation helpers ---\n",
    "def random_brightness_contrast(image):\n",
    "    # image assumed float32 in [0,255]\n",
    "    alpha = np.random.uniform(0.85, 1.15)\n",
    "    beta = np.random.uniform(-0.1, 0.1) * 255.0\n",
    "    out = image * alpha + beta\n",
    "    return np.clip(out, 0.0, 255.0)\n",
    "\n",
    "def safe_center_crop_resize(img, target_size):\n",
    "    # ensure image at least target size by padding if necessary, then center-crop\n",
    "    h, w = img.shape[:2]\n",
    "    if h < target_size or w < target_size:\n",
    "        pad_h = max(0, target_size - h)\n",
    "        pad_w = max(0, target_size - w)\n",
    "        top = pad_h // 2\n",
    "        bottom = pad_h - top\n",
    "        left = pad_w // 2\n",
    "        right = pad_w - left\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REFLECT)\n",
    "        h, w = img.shape[:2]\n",
    "    startx = (w - target_size) // 2\n",
    "    starty = (h - target_size) // 2\n",
    "    cropped = img[starty:starty+target_size, startx:startx+target_size]\n",
    "    return cv2.resize(cropped, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def random_flip_rotate_scale_crop(img):\n",
    "    # expects img float32 in [0,255]\n",
    "    if random.random() < 0.5:\n",
    "        img = cv2.flip(img, 1)\n",
    "    # rotate\n",
    "    angle = random.uniform(-15, 15)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1.0)\n",
    "    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REFLECT)\n",
    "    # scale\n",
    "    scale = random.uniform(0.9, 1.1)\n",
    "    new_w = max(1, int(img.shape[1] * scale))\n",
    "    new_h = max(1, int(img.shape[0] * scale))\n",
    "    img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    img = safe_center_crop_resize(img, IMG_SIZE)\n",
    "    return img\n",
    "\n",
    "# --- Sequence implementation ---\n",
    "class MangosteenSequence(Sequence):\n",
    "    def __init__(self, image_paths, volumes, batch_size, img_size, is_training=True, shuffle=True):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.volumes = list(volumes)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.is_training = is_training\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.image_paths) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = min(start + self.batch_size, len(self.image_paths))\n",
    "        batch_paths = self.image_paths[start:end]\n",
    "        batch_vols = self.volumes[start:end]\n",
    "\n",
    "        images = np.zeros((len(batch_paths), self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        for i, p in enumerate(batch_paths):\n",
    "            img = cv2.imread(p)\n",
    "            if img is None:\n",
    "                logging.warning(f\"cv2.imread failed for {p}. Using black image.\")\n",
    "                img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "            img = img.astype('float32')\n",
    "            if self.is_training:\n",
    "                if random.random() < 0.9:\n",
    "                    img = random_brightness_contrast(img)\n",
    "                if random.random() < 0.7:\n",
    "                    img = random_flip_rotate_scale_crop(img)\n",
    "                # small gaussian noise occasionally\n",
    "                if random.random() < 0.2:\n",
    "                    noise = np.random.normal(0, 2.0, img.shape).astype(np.float32)\n",
    "                    img = np.clip(img + noise, 0, 255)\n",
    "            images[i] = img / 255.0\n",
    "        vols = np.array(batch_vols, dtype='float32')\n",
    "        return images, vols\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle and self.is_training and len(self.image_paths) > 1:\n",
    "            combined = list(zip(self.image_paths, self.volumes))\n",
    "            random.shuffle(combined)\n",
    "            self.image_paths, self.volumes = zip(*combined)\n",
    "            self.image_paths = list(self.image_paths)\n",
    "            self.volumes = list(self.volumes)\n",
    "\n",
    "# --- load data ---\n",
    "def load_data_from_folders(data_dir, volume_dir):\n",
    "    image_paths = []\n",
    "    volumes = []\n",
    "    missing = 0\n",
    "    bad_files = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            base = os.path.splitext(filename)[0]\n",
    "            vol_file = os.path.join(volume_dir, base + \".txt\")\n",
    "            if os.path.exists(vol_file):\n",
    "                try:\n",
    "                    with open(vol_file, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                        v = float(text)\n",
    "                    image_paths.append(os.path.join(data_dir, filename))\n",
    "                    volumes.append(v)\n",
    "                except Exception as e:\n",
    "                    bad_files.append((vol_file, str(e)))\n",
    "            else:\n",
    "                missing += 1\n",
    "    if missing:\n",
    "        logging.info(f\"{missing} images found without matching .txt label files.\")\n",
    "    if bad_files:\n",
    "        logging.warning(f\"{len(bad_files)} label files failed to parse. Examples: {bad_files[:3]}\")\n",
    "    return image_paths, volumes\n",
    "\n",
    "# ---------------- main pipeline ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading training data...\")\n",
    "    image_paths, volumes = load_data_from_folders(DATA_DIR, VOLUME_FILES_DIR)\n",
    "    if not image_paths:\n",
    "        raise SystemExit(\"❌ No data found. ตรวจสอบ DATA_DIR และ VOLUME_FILES_DIR\")\n",
    "\n",
    "    # split first to avoid leakage\n",
    "    train_paths, val_paths, train_vols, val_vols = train_test_split(\n",
    "        image_paths, volumes, test_size=0.2, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # fit scaler on train only\n",
    "    scaler = StandardScaler()\n",
    "    train_vols_arr = np.array(train_vols).reshape(-1, 1)\n",
    "    val_vols_arr = np.array(val_vols).reshape(-1, 1)\n",
    "    scaler.fit(train_vols_arr)\n",
    "    train_vols_scaled = scaler.transform(train_vols_arr).flatten()\n",
    "    val_vols_scaled = scaler.transform(val_vols_arr).flatten()\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    logging.info(f\"Scaler saved to {SCALER_PATH}\")\n",
    "\n",
    "    # sequences\n",
    "    train_seq = MangosteenSequence(train_paths, train_vols_scaled, BATCH_SIZE, IMG_SIZE, is_training=True)\n",
    "    val_seq = MangosteenSequence(val_paths, val_vols_scaled, BATCH_SIZE, IMG_SIZE, is_training=False, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    # if mixed precision, last Dense should output float32 for stable loss computation\n",
    "    out = Dense(1, activation='linear', dtype='float32')(x) if ENABLE_MIXED_PRECISION else Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
    "                  loss='huber', metrics=['mean_absolute_error'])\n",
    "\n",
    "    # callbacks\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"best_{now}.h5\")\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, min_lr=1e-6, verbose=1),\n",
    "        TensorBoard(log_dir=os.path.join(\"logs\", now)),\n",
    "        CSVLogger(os.path.join(\"logs\", f\"training_{now}.csv\"))\n",
    "    ]\n",
    "\n",
    "    # train regression head\n",
    "    logging.info(\"Training regression head...\")\n",
    "    history1 = model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # fine-tune last block(s)\n",
    "    # Xception block14 is named like 'block14...'; unlock some of the deeper layers\n",
    "    for layer in base_model.layers:\n",
    "        if 'block14' in layer.name or 'block13' in layer.name:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
    "                  loss='huber', metrics=['mean_absolute_error'])\n",
    "\n",
    "    # --- fine-tune last block(s) (แก้: เอา workers/use_multiprocessing ออก) ---\n",
    "    logging.info(\"Fine-tuning last blocks of base model...\")\n",
    "    history2 = model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=FINE_TUNE_EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    # save final model\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "    logging.info(f\"✅ Model saved to {MODEL_SAVE_PATH}\")\n",
    "    logging.info(f\"Best checkpoint: {checkpoint_path}\")\n",
    "    # ---- plot training history ----\n",
    "    def plot_history(h1, h2, filename=\"training_plot.png\"):\n",
    "        df1 = pd.DataFrame(h1.history) if h1 else None\n",
    "        df2 = pd.DataFrame(h2.history) if h2 else None\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        # loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if df1 is not None:\n",
    "            plt.plot(df1['loss'], label='train_loss_phase1')\n",
    "            plt.plot(df1['val_loss'], label='val_loss_phase1')\n",
    "        if df2 is not None:\n",
    "            plt.plot(df2['loss'], label='train_loss_phase2')\n",
    "            plt.plot(df2['val_loss'], label='val_loss_phase2')\n",
    "        plt.legend(); plt.title('Loss')\n",
    "        # mae\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if df1 is not None:\n",
    "            plt.plot(df1['mean_absolute_error'], label='train_mae_phase1')\n",
    "            plt.plot(df1['val_mean_absolute_error'], label='val_mae_phase1')\n",
    "        if df2 is not None:\n",
    "            plt.plot(df2['mean_absolute_error'], label='train_mae_phase2')\n",
    "            plt.plot(df2['val_mean_absolute_error'], label='val_mae_phase2')\n",
    "        plt.legend(); plt.title('MAE')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        logging.info(f\"Training plot saved to {filename}\")\n",
    "    plot_history(history1, history2, filename=f\"training_{now}.png\")\n",
    "    # ---- evaluate on validation in original units ----\n",
    "    logging.info(\"Evaluating on validation set (original volume units)...\")\n",
    "    # predict all val (iterate val_seq)\n",
    "    preds_scaled = []\n",
    "    trues_scaled = []\n",
    "    for Xb, yb in val_seq:\n",
    "        p = model.predict(Xb, verbose=0)\n",
    "        preds_scaled.append(p.reshape(-1))\n",
    "        trues_scaled.append(yb.reshape(-1))\n",
    "    preds_scaled = np.concatenate(preds_scaled, axis=0)\n",
    "    trues_scaled = np.concatenate(trues_scaled, axis=0)\n",
    "    # inverse transform\n",
    "    preds_orig = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "    trues_orig = scaler.inverse_transform(trues_scaled.reshape(-1, 1)).flatten()\n",
    "    mae_val = mean_absolute_error(trues_orig, preds_orig)\n",
    "    rmse_val = math.sqrt(mean_squared_error(trues_orig, preds_orig))\n",
    "    logging.info(f\"Validation MAE (orig units): {mae_val:.4f}\")\n",
    "    logging.info(f\"Validation RMSE (orig units): {rmse_val:.4f}\")\n",
    "    # save a small csv with true vs pred\n",
    "    out_df = pd.DataFrame({\"path\": val_paths[:len(preds_orig)], \"true\": trues_orig, \"pred\": preds_orig})\n",
    "    out_csv = f\"val_predictions_{now}.csv\"\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    logging.info(f\"Saved validation predictions to {out_csv}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d830771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:52:35,338 INFO: Fine-tuning last blocks of base model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3462 - mean_absolute_error: 0.5799\n",
      "Epoch 1: val_loss improved from 0.26467 to 0.26382, saving model to ./checkpoints\\best_20250913-210813.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:52:53,667 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.3161 - mean_absolute_error: 0.5441 - val_loss: 0.2638 - val_mean_absolute_error: 0.4774 - learning_rate: 1.0000e-06\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2996 - mean_absolute_error: 0.5262\n",
      "Epoch 2: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - loss: 0.2999 - mean_absolute_error: 0.5268 - val_loss: 0.2645 - val_mean_absolute_error: 0.4792 - learning_rate: 1.0000e-06\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2823 - mean_absolute_error: 0.5033\n",
      "Epoch 3: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.3059 - mean_absolute_error: 0.5296 - val_loss: 0.2658 - val_mean_absolute_error: 0.4799 - learning_rate: 1.0000e-06\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2946 - mean_absolute_error: 0.5178\n",
      "Epoch 4: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2994 - mean_absolute_error: 0.5308 - val_loss: 0.2673 - val_mean_absolute_error: 0.4807 - learning_rate: 1.0000e-06\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3055 - mean_absolute_error: 0.5412\n",
      "Epoch 5: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2951 - mean_absolute_error: 0.5225 - val_loss: 0.2690 - val_mean_absolute_error: 0.4835 - learning_rate: 1.0000e-06\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2753 - mean_absolute_error: 0.4979\n",
      "Epoch 6: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 0.2884 - mean_absolute_error: 0.5116 - val_loss: 0.2714 - val_mean_absolute_error: 0.4878 - learning_rate: 1.0000e-06\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2731 - mean_absolute_error: 0.5050\n",
      "Epoch 7: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.2827 - mean_absolute_error: 0.5073 - val_loss: 0.2741 - val_mean_absolute_error: 0.4928 - learning_rate: 1.0000e-06\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3007 - mean_absolute_error: 0.5209\n",
      "Epoch 8: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 0.2847 - mean_absolute_error: 0.4995 - val_loss: 0.2765 - val_mean_absolute_error: 0.4973 - learning_rate: 1.0000e-06\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.3027 - mean_absolute_error: 0.5328\n",
      "Epoch 9: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - loss: 0.2751 - mean_absolute_error: 0.4986 - val_loss: 0.2797 - val_mean_absolute_error: 0.5021 - learning_rate: 1.0000e-06\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2931 - mean_absolute_error: 0.5128\n",
      "Epoch 10: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - loss: 0.2734 - mean_absolute_error: 0.4921 - val_loss: 0.2832 - val_mean_absolute_error: 0.5082 - learning_rate: 1.0000e-06\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3063 - mean_absolute_error: 0.5458\n",
      "Epoch 11: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2765 - mean_absolute_error: 0.4967 - val_loss: 0.2864 - val_mean_absolute_error: 0.5138 - learning_rate: 1.0000e-06\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2769 - mean_absolute_error: 0.5070\n",
      "Epoch 12: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2730 - mean_absolute_error: 0.4970 - val_loss: 0.2896 - val_mean_absolute_error: 0.5192 - learning_rate: 1.0000e-06\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2927 - mean_absolute_error: 0.5239\n",
      "Epoch 13: val_loss did not improve from 0.26382\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.2668 - mean_absolute_error: 0.4823 - val_loss: 0.2922 - val_mean_absolute_error: 0.5233 - learning_rate: 1.0000e-06\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:56:39,556 WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-09-13 21:56:39,909 INFO: ✅ Model saved to ./mangosteen_volume_model_aug_finetune.h5\n",
      "2025-09-13 21:56:39,911 INFO: Best checkpoint: ./checkpoints\\best_20250913-210813.h5\n",
      "2025-09-13 21:56:40,106 INFO: Training plot saved to training_20250913-210813.png\n",
      "2025-09-13 21:56:40,106 INFO: Evaluating on validation set (original volume units)...\n",
      "2025-09-13 21:56:43,383 INFO: Validation MAE (orig units): 12.2008\n",
      "2025-09-13 21:56:43,384 INFO: Validation RMSE (orig units): 17.6673\n",
      "2025-09-13 21:56:43,386 INFO: Saved validation predictions to val_predictions_20250913-210813.csv\n"
     ]
    }
   ],
   "source": [
    "# --- fine-tune last block(s) (แก้: เอา workers/use_multiprocessing ออก) ---\n",
    "logging.info(\"Fine-tuning last blocks of base model...\")\n",
    "history2 = model.fit(\n",
    "    train_seq,\n",
    "    validation_data=val_seq,\n",
    "    epochs=FINE_TUNE_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "# save final model\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "logging.info(f\"✅ Model saved to {MODEL_SAVE_PATH}\")\n",
    "logging.info(f\"Best checkpoint: {checkpoint_path}\")\n",
    "# ---- plot training history ----\n",
    "def plot_history(h1, h2, filename=\"training_plot.png\"):\n",
    "    df1 = pd.DataFrame(h1.history) if h1 else None\n",
    "    df2 = pd.DataFrame(h2.history) if h2 else None\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if df1 is not None:\n",
    "        plt.plot(df1['loss'], label='train_loss_phase1')\n",
    "        plt.plot(df1['val_loss'], label='val_loss_phase1')\n",
    "    if df2 is not None:\n",
    "        plt.plot(df2['loss'], label='train_loss_phase2')\n",
    "        plt.plot(df2['val_loss'], label='val_loss_phase2')\n",
    "    plt.legend(); plt.title('Loss')\n",
    "    # mae\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if df1 is not None:\n",
    "        plt.plot(df1['mean_absolute_error'], label='train_mae_phase1')\n",
    "        plt.plot(df1['val_mean_absolute_error'], label='val_mae_phase1')\n",
    "    if df2 is not None:\n",
    "        plt.plot(df2['mean_absolute_error'], label='train_mae_phase2')\n",
    "        plt.plot(df2['val_mean_absolute_error'], label='val_mae_phase2')\n",
    "    plt.legend(); plt.title('MAE')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    logging.info(f\"Training plot saved to {filename}\")\n",
    "plot_history(history1, history2, filename=f\"training_{now}.png\")\n",
    "# ---- evaluate on validation in original units ----\n",
    "logging.info(\"Evaluating on validation set (original volume units)...\")\n",
    "# predict all val (iterate val_seq)\n",
    "preds_scaled = []\n",
    "trues_scaled = []\n",
    "for Xb, yb in val_seq:\n",
    "    p = model.predict(Xb, verbose=0)\n",
    "    preds_scaled.append(p.reshape(-1))\n",
    "    trues_scaled.append(yb.reshape(-1))\n",
    "preds_scaled = np.concatenate(preds_scaled, axis=0)\n",
    "trues_scaled = np.concatenate(trues_scaled, axis=0)\n",
    "# inverse transform\n",
    "preds_orig = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "trues_orig = scaler.inverse_transform(trues_scaled.reshape(-1, 1)).flatten()\n",
    "mae_val = mean_absolute_error(trues_orig, preds_orig)\n",
    "rmse_val = math.sqrt(mean_squared_error(trues_orig, preds_orig))\n",
    "logging.info(f\"Validation MAE (orig units): {mae_val:.4f}\")\n",
    "logging.info(f\"Validation RMSE (orig units): {rmse_val:.4f}\")\n",
    "# save a small csv with true vs pred\n",
    "out_df = pd.DataFrame({\"path\": val_paths[:len(preds_orig)], \"true\": trues_orig, \"pred\": preds_orig})\n",
    "out_csv = f\"val_predictions_{now}.csv\"\n",
    "out_df.to_csv(out_csv, index=False)\n",
    "logging.info(f\"Saved validation predictions to {out_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
